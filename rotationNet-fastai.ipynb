{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONBREAKPOINT=IPython.core.debugger.set_trace\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%set_env PYTHONBREAKPOINT=IPython.core.debugger.set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcand = np.load('vcand_case2.npy')\n",
    "nview = 20\n",
    "viewOrd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('ModelNet40_20/val'),\n",
       " PosixPath('ModelNet40_20/test'),\n",
       " PosixPath('ModelNet40_20/train'),\n",
       " PosixPath('ModelNet40_20/item_list.txt'),\n",
       " PosixPath('ModelNet40_20/.ipynb_checkpoints'),\n",
       " PosixPath('ModelNet40_20/models')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./ModelNet40_20')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(os.listdir(path/'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the validation dataset, count the number of data points for each class.\n",
    "# Used to calculate accuracy-per-class during validation.\n",
    "\n",
    "def directory(path,extension):\n",
    "    list_dir = []\n",
    "    list_dir = os.listdir(path)\n",
    "    count = 0\n",
    "    for file in list_dir:\n",
    "        if file.endswith(extension): # eg: '.txt'\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "folders = [f.path for f in os.scandir(path/'test') if f.is_dir()]\n",
    "folders = sorted(folders)\n",
    "validCount = []\n",
    "for folds in folders:\n",
    "    validCount = np.append(validCount, directory(folds, '.png')//20)\n",
    "validCount = validCount.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = np.arange(0, 40)\n",
    "# counts = np.random.randint(low=0, high = 100, size = 40)\n",
    "# v = [validCount[i] for i in u.astype(int)]\n",
    "# unique = [classes[i] for i in u.astype(int)]\n",
    "# corrects = ((counts/v * 100) + 0.5).astype(int) / 100.0\n",
    "# countDict = dict(zip(unique, corrects))\n",
    "# countDict\n",
    "\n",
    "#         u, counts = np.unique(self.incTargs, return_counts=True)\n",
    "#         v = [validCount[i] for i in u.astype(int)]\n",
    "#         u = [classes[i] for i in u.astype(int)]\n",
    "#         countDict = dict(zip(u, (counts, v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = get_transforms(do_flip=False, flip_vert=False, max_rotate=0.0,\n",
    "                      max_lighting=0.0, max_zoom=1.05, max_warp=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelLists;\n",
       "\n",
       "Train: LabelList (196860 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "mantel,mantel,mantel,mantel,mantel\n",
       "Path: ModelNet40_20;\n",
       "\n",
       "Valid: LabelList (49360 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "mantel,mantel,mantel,mantel,mantel\n",
       "Path: ModelNet40_20;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = (ImageList.from_folder(path)\n",
    "      .split_by_folder(train='train', valid='test')\n",
    "      .label_from_folder())\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(tfms, size=128)\n",
    "        .databunch(bs=400).normalize(imagenet_stats))\n",
    "\n",
    "data.train_dl = data.train_dl.new(shuffle=False)\n",
    "data.valid_dl = data.valid_dl.new(shuffle=False)\n",
    "\n",
    "if data.train_dl.dl.batch_size % nview != 0:\n",
    "    print ('Error: batch size must be a multiplication of the number of views,', nview)\n",
    "    exit()\n",
    "    \n",
    "sorted_img = sorted(data.train_ds.x.items)\n",
    "sorted_cats = sorted(data.train_ds.y.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47820 47821 47822 47823 ...  1756  1757  1758  1759]\n"
     ]
    }
   ],
   "source": [
    "# random permutation for validation set. Probably not necessary, but just an experiment to see if it makes\n",
    "# any difference during training/validation.\n",
    "sorted_vimg = sorted(data.valid_ds.x.items)\n",
    "sorted_vcats = sorted(data.valid_ds.y.items)\n",
    "\n",
    "val_nsamp = int( len(data.valid_ds) / nview )\n",
    "\n",
    "inds = np.zeros( ( nview, val_nsamp ) ).astype('int')\n",
    "inds[ 0 ] = np.random.permutation(range(val_nsamp)) * nview\n",
    "for i in range(1,nview):\n",
    "    inds[ i ] = inds[ 0 ] + i\n",
    "inds = inds.T.reshape( nview * val_nsamp )\n",
    "print(inds)\n",
    "\n",
    "IL  = ImageList([sorted_vimg[i] for i in inds], path=path)\n",
    "data.valid_ds.x = IL\n",
    "CL = CategoryList([sorted_vcats[i] for i in inds], classes=classes, path=path)\n",
    "data.valid_ds.y = CL\n",
    "# data.valid_ds.x = ImageList(sorted(data.valid_ds.x.items), path=path)\n",
    "# data.valid_ds.y = CategoryList(sorted(data.valid_ds.y.items), classes=classes, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = random.randint(0, 50000)\n",
    "# print(data.valid_ds[idx][1], idx)\n",
    "# data.valid_ds.items[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList (49360 items)\n",
       "x: ImageList\n",
       "Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)\n",
       "y: CategoryList\n",
       "vase,vase,vase,vase,vase\n",
       "Path: ModelNet40_20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 8779\n",
    "# print(data.train_ds[idx])\n",
    "# data.train_ds[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class updateTargetRN(LearnerCallback):\n",
    "#     _order=10\n",
    "#     \"Overwrites the target labels for the rotationNet implementation.\"\n",
    "#     def on_batch_begin(self, last_input:Tensor, last_target:Tensor, train:bool=True, **kwargs)->None:\n",
    "#         print('Entering updateTargetRN!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FineTuneModel(nn.Module):\n",
    "#     def __init__(self, original_model, arch, num_classes):\n",
    "#         super(FineTuneModel, self).__init__()\n",
    "#         self.features = original_model.features\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(4096, num_classes),\n",
    "#         )\n",
    "        \n",
    "#     # Freeze those weights\n",
    "# #     for p in self.features.parameters():\n",
    "# #         p.requires_grad = False\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         f = self.features(x)\n",
    "#         f = f.view(f.size(0), 256 * 6 * 6)\n",
    "#         y = self.classifier(f)\n",
    "#         return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=True)\n",
    "# model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "# model = FineTuneModel(model, 'resnet', numClasses )\n",
    "# model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateTargetrN(LearnerCallback):\n",
    "\n",
    "    \"Creates the target labels for a rotationNet implementation.\"\n",
    "    \n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        self.target_var = torch.LongTensor( learn.data.train_dl.dl.batch_size * nview )\n",
    "        self.loss_func = learn.loss_func\n",
    "        self.output = torch.Tensor()\n",
    "        \n",
    "#     def on_batch_begin(self, last_input, last_target, **kwargs)->None:\n",
    "#         print ('Entering on_batch_begin!')\n",
    "#         img = last_input[0].clone().cpu()\n",
    "#         img = img.numpy()\n",
    "#         img = img.transpose(1,2,0)\n",
    "#         img = abs(img*85).astype(int)\n",
    "#         imgplot = plt.imshow(img)\n",
    "        \n",
    "    def on_loss_begin(self, last_output, last_target, train, **kwargs:Any)->None:\n",
    "        \n",
    "        if not train:\n",
    "            return {'last_output':last_output, 'last_target':last_target}\n",
    "\n",
    "        nsamp = int( last_output.size(0) / nview )\n",
    "#         print('Number of samples: ', nsamp)\n",
    "\n",
    "#         input_var = torch.autograd.Variable(input)\n",
    "        target_ = torch.LongTensor( last_target.size(0) * nview )\n",
    "#         print('Last_target: ', last_target, last_target.shape, target_.shape)\n",
    "\n",
    "        # compute output\n",
    "        self.output = last_output\n",
    "#         print('self.output initial shape: ', self.output.shape)\n",
    "        num_classes = int( self.output.size( 1 ) / nview ) - 1\n",
    "#         print('Number of classes: ', num_classes)\n",
    "        self.output = self.output.view( -1, num_classes + 1 )\n",
    "#         print('self.output after reshaping: ', self.output)\n",
    "\n",
    "        # compute scores and decide target labels\n",
    "        output_ = torch.nn.functional.log_softmax( self.output, dim=1 ) #try sigmoid instead?\n",
    "#         print('output_ after sofmax: ', output_.shape)\n",
    "\n",
    "        #subtracts the last col from every other col, removes last col\n",
    "        output_ = output_[ :, :-1 ] - torch.t( output_[ :, -1 ].repeat(1, output_.size(1)-1).view(output_.size(1)-1, -1) )\n",
    "#         print('output_ after subtracting last column', output_.shape)\n",
    "        output_ = output_.view( -1, nview * nview, num_classes )\n",
    "#         print('output_', output_)\n",
    "        output_ = output_.data.cpu().numpy()\n",
    "\n",
    "        '''Arrange as:\n",
    "           [x,:,:] = all views activations for image[x//num views]\n",
    "           [:,y,:] = view activations for class[y]\n",
    "           [:,:,z] = sample set[z]'''\n",
    "\n",
    "        output_ = output_.transpose( 1, 2, 0 )\n",
    "#         print('output_: ', output_)\n",
    "\n",
    "        #default view variable is the incorrect view\n",
    "        for j in range(target_.size(0)):\n",
    "            target_[ j ] = num_classes\n",
    "\n",
    "        # Initialize scores to 0\n",
    "        scores = np.zeros( ( vcand.shape[ 0 ], num_classes, nsamp ) )\n",
    "\n",
    "        #add up scores for each of the candidates for viewpoint variables\n",
    "        for j in range(vcand.shape[0]):\n",
    "            for k in range(vcand.shape[1]):\n",
    "                scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]\n",
    "\n",
    "        for n in range( nsamp ):\n",
    "            #finds max score for column that corresponds to target label (for each class label and sample group)\n",
    "            j_max = np.argmax( scores[ :, last_target[ n * nview ], n ] )\n",
    "            # Assign target labels. Only 1 view per image gets set to class label.\n",
    "            # Others remain default, which is the incorrect view\n",
    "            for k in range(vcand.shape[1]):\n",
    "                target_[ n * nview * nview + vcand[ j_max ][ k ] * nview + k ] = last_target[ n * nview ]\n",
    "\n",
    "#         print('Final targets: ', target_.shape)\n",
    "\n",
    "        target_ = target_.cuda()\n",
    "        self.target_var = torch.autograd.Variable(target_)\n",
    "#         print('self.target_var: ', self.target_var)\n",
    "\n",
    "#             print(list(learn.model.parameters())[-4][:5])\n",
    "\n",
    "        return {'last_output':self.output, 'last_target':self.target_var}\n",
    "    \n",
    "#     def on_backward_begin(self, **kwargs):\n",
    "# #         print(\"Entering on_backward_begin!\")\n",
    "# #         print('Last_loss: ', last_loss)\n",
    "#         if not is_listy(self.target_var):\n",
    "#             loss = self.loss_func(self.output, *[self.target_var])\n",
    "#         else:\n",
    "#             loss = self.loss_func(self.output, *self.target_var)\n",
    "# #         print('Updated loss: ', loss)\n",
    "#         return {'last_loss':loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rNAccuracy(LearnerCallback):\n",
    "    _order=-20\n",
    "    def __init__(self, learn):\n",
    "        super().__init__(learn)\n",
    "  \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.learn.recorder.add_metric_names(['rN_t_loss', 'Prec@1', 'Prec@5'])\n",
    "        self.vcount_tot = 0\n",
    "        self.prec1, self.prec5 = 0., 0.\n",
    "        \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.tloss_tot, self.vloss_tot, self.tcount, self.vcount= 0., 0., 0, 0\n",
    "        self.incTargs = []\n",
    "        self.viewOrd = []\n",
    "        \n",
    "        #print weights to confirm body is frozen or not\n",
    "#         print(list(learn.model.parameters())[0][0][0][1])\n",
    "        \n",
    "        # random permutation\n",
    "        train_nsamp = int( len(learn.data.train_ds) / nview )\n",
    "        \n",
    "        inds = np.zeros( ( nview, train_nsamp ) ).astype('int')\n",
    "        inds[ 0 ] = np.random.permutation(range(train_nsamp)) * nview\n",
    "        for i in range(1,nview):\n",
    "            inds[ i ] = inds[ 0 ] + i\n",
    "        inds = inds.T.reshape( nview * train_nsamp )\n",
    "        \n",
    "        IL  = ImageList([sorted_img[i] for i in inds], path=path)\n",
    "        learn.data.train_ds.x = IL\n",
    "        CL = CategoryList([sorted_cats[i] for i in inds], classes=classes, path=path)\n",
    "        learn.data.train_ds.y = CL\n",
    "        # Print a random data item to make sure they got shuffled correctly\n",
    "#         idx = random.randint(0,len(learn.data.train_ds))\n",
    "#         print(data.train_ds[idx][1], data.train_ds.items[idx])\n",
    "\n",
    "\n",
    "    def on_batch_end(self, last_loss, last_output, last_target, train, **kwargs):\n",
    "        \n",
    "        if train:\n",
    "            self.tloss_tot += last_loss\n",
    "            self.tcount += 1\n",
    "        else:\n",
    "#             print('Entering on_batch_end for valididation.')\n",
    "            self.vloss_tot += last_loss\n",
    "            self.vcount += 1\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            \n",
    "            output_ = last_output\n",
    "#             print('initial output_: ', output_.shape)\n",
    "            target = last_target\n",
    "            target = target.cuda()\n",
    "            target = target[0:-1:nview]\n",
    "#             print('target:', target, target.shape)\n",
    "            nsamp = int( output_.size(0) / nview )\n",
    "#             print('nsamp: ', nsamp)\n",
    "            batch_size = target.size(0)\n",
    "#             print('batch_size: ', batch_size)\n",
    "\n",
    "            num_classes = int(output_.size( 1 )/ nview) - 1\n",
    "#             print('num_classes: ', num_classes)\n",
    "            output_ = output_.view( -1, num_classes + 1 )\n",
    "#             print('output_: ', output_)\n",
    "            output_ = torch.nn.functional.log_softmax( output_, dim=1 ) #try sigmoid instead?\n",
    "            output_ = output_[ :, :-1 ] - torch.t( output_[ :, -1 ].repeat(1, output_.size(1)-1).view(output_.size(1)-1, -1) )\n",
    "            output_ = output_.view( -1, nview * nview, num_classes )\n",
    "            output_ = output_.data.cpu().numpy()\n",
    "\n",
    "            output_ = output_.transpose( 1, 2, 0 )\n",
    "#             print('output_: ', output_.shape)\n",
    "            scores = np.zeros( ( vcand.shape[ 0 ], num_classes, batch_size ) )\n",
    "#             print('scores: ', scores.shape)\n",
    "            output = torch.zeros( ( batch_size, num_classes ) )\n",
    "    #         print('output: ', output)\n",
    "    \n",
    "            for j in range(vcand.shape[0]):\n",
    "                for k in range(vcand.shape[1]):\n",
    "                    scores[ j ] = scores[ j ] + output_[ vcand[ j ][ k ] * nview + k ]\n",
    "#             print('scores: ', scores)\n",
    "\n",
    "            for n in range( batch_size ):\n",
    "                # For a given batch, n, np.argmax( scores[ :, :, n ] ) / scores.shape[ 1 ]\n",
    "                # gets the index for the view that has the highest score, regardless of class.\n",
    "                j_max = int( np.argmax( scores[ :, :, n ] ) / scores.shape[ 1 ] )\n",
    "                self.viewOrd = np.append(self.viewOrd, j_max)\n",
    "                # Scores[ j_max, :, n ] -- for batch n, view with highest score regardless of class.\n",
    "                output[ n ] = torch.FloatTensor( scores[ j_max, :, n ] )\n",
    "#                 print('scores[j_max,:,n]: ', scores[ j_max, :, n ], j_max)\n",
    "\n",
    "            # For each batch, the scores for the views that had the highest score.\n",
    "#             print('output:', output)\n",
    "            output = output.cuda()\n",
    "            # output[x,:] - view that had the highest class score for sample[x]\n",
    "            # output[:,y] - score for class[y]\n",
    "\n",
    "            topk = (1,5)\n",
    "            maxk = max(topk)\n",
    "            _, pred = output.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "#             print('pred/target: ', pred, target)\n",
    "#             _, pred40 = output.topk(40, 1, True, True)\n",
    "\n",
    "\n",
    "            correct = pred.eq(target.contiguous().view(1, -1).expand_as(pred))\n",
    "\n",
    "            prec = []\n",
    "            for k in topk:\n",
    "                correct_k = correct[:k].view(-1).float().sum(0)\n",
    "                prec.append(correct_k.mul_(100.0 / batch_size))\n",
    "                \n",
    "            self.prec1 += prec[0]*(last_output.size(0)//nview)\n",
    "            self.prec5 += prec[1]*(last_output.size(0)//nview)\n",
    "            self.vcount_tot += (last_output.size(0)//nview)\n",
    "            \n",
    "            idx =  (correct[0] == 0).nonzero()\n",
    "#             print(correct[0], idx)\n",
    "            self.incTargs = np.append(self.incTargs, target[idx.view(-1)].data.cpu().numpy())\n",
    "#             print(self.incTargs)\n",
    "            \n",
    "#             if prec[0] < 85.0:\n",
    "# #                 print('prec1, prec5: ', prec)\n",
    "# #                 print('\\npred: ', pred[0])\n",
    "#                 idx =  (correct[0] == 0).nonzero()\n",
    "# #                 print(correct[0], idx.view(-1))\n",
    "#                 print('Validation targets below 85%: ', target[idx.view(-1)], prec)\n",
    "\n",
    "#             print('self.vcount_tot: ', self.vcount_tot)\n",
    "            return {'last_output':output}\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, epoch, **kwargs):\n",
    "        \n",
    "        global viewOrd\n",
    "        viewOrd = self.viewOrd.astype(int)\n",
    "        \n",
    "        print('\\n*******EPOCH', epoch, '********\\n')\n",
    "        print('\\nviewOrd: ', viewOrd, len(viewOrd))\n",
    "        print('Error rate by class:')\n",
    "        # Count classes that have been incorrectly classified\n",
    "        u, counts = np.unique(self.incTargs, return_counts=True)\n",
    "        v = [validCount[i] for i in u.astype(int)]\n",
    "        unique = [classes[i] for i in u.astype(int)]\n",
    "        corrects = ((counts/v * 100) + 0.5).astype(int) / 100.0\n",
    "        countDict = dict(zip(unique, corrects))\n",
    "        print(countDict, len(countDict))\n",
    "        \n",
    "\n",
    "#         print('vcount:', self.vcount)\n",
    "        return add_metrics(last_metrics, [self.tloss_tot/self.tcount,\n",
    "                                          self.prec1/self.vcount_tot, self.prec5/self.vcount_tot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = (data.c+1)*nview\n",
    "rotationNetHead = create_head(512, numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, models.alexnet, metrics=None, pretrained=True,\n",
    "                    custom_head = rotationNetHead, callback_fns=[GenerateTargetrN, rNAccuracy])\n",
    "# learn = cnn_learner(data, models.resnet18, ps = 0.6, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:525px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>rN_t_loss</th>\n",
       "    <th>Prec@1</th>\n",
       "    <th>Prec@5</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='492', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m(26)\u001b[0;36mloss_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0;31m#if not is_listy(yb): yb = [yb]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m    \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m    \u001b[0;31m# If training, update both out and target before finding loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> xb\n",
      "[tensor([[[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]],\n",
      "       device='cuda:0')]\n",
      "ipdb> xb.shape\n",
      "*** AttributeError: 'list' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> len(xb)\n",
      "1\n",
      "ipdb> xb[0]\n",
      "tensor([[[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]],\n",
      "\n",
      "\n",
      "        [[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          ...,\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          ...,\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          ...,\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]],\n",
      "       device='cuda:0')\n",
      "ipdb> xb[0].shape\n",
      "torch.Size([400, 3, 128, 128])\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-64cb08811a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#if not is_listy(yb): yb = [yb]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# If training, update both out and target before finding loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#if not is_listy(yb): yb = [yb]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# If training, update both out and target before finding loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save('stage_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage_1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 55:18 <p><table style='width:525px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>rN_t_loss</th>\n",
       "    <th>Prec@1</th>\n",
       "    <th>Prec@5</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>0.088160</th>\n",
       "    <th>17.557230</th>\n",
       "    <th>0.089666</th>\n",
       "    <th>90.964340</th>\n",
       "    <th>97.163696</th>\n",
       "    <th>02:45</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.083081</th>\n",
       "    <th>18.347692</th>\n",
       "    <th>0.082859</th>\n",
       "    <th>90.863045</th>\n",
       "    <th>97.163696</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.074197</th>\n",
       "    <th>19.106340</th>\n",
       "    <th>0.077210</th>\n",
       "    <th>90.869804</th>\n",
       "    <th>97.285255</th>\n",
       "    <th>02:45</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.074255</th>\n",
       "    <th>18.841602</th>\n",
       "    <th>0.072883</th>\n",
       "    <th>91.055511</th>\n",
       "    <th>97.325768</th>\n",
       "    <th>02:47</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.069703</th>\n",
       "    <th>19.650620</th>\n",
       "    <th>0.068665</th>\n",
       "    <th>91.264183</th>\n",
       "    <th>97.390602</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.065499</th>\n",
       "    <th>20.273008</th>\n",
       "    <th>0.064268</th>\n",
       "    <th>91.410049</th>\n",
       "    <th>97.406807</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>0.059964</th>\n",
       "    <th>20.106440</th>\n",
       "    <th>0.059495</th>\n",
       "    <th>91.548965</th>\n",
       "    <th>97.476265</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>0.054735</th>\n",
       "    <th>19.531006</th>\n",
       "    <th>0.054830</th>\n",
       "    <th>91.693680</th>\n",
       "    <th>97.503036</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>0.050273</th>\n",
       "    <th>21.293341</th>\n",
       "    <th>0.051032</th>\n",
       "    <th>91.842247</th>\n",
       "    <th>97.537361</th>\n",
       "    <th>02:47</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>0.047539</th>\n",
       "    <th>20.947386</th>\n",
       "    <th>0.047211</th>\n",
       "    <th>92.034042</th>\n",
       "    <th>97.593193</th>\n",
       "    <th>02:45</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>0.044013</th>\n",
       "    <th>21.603077</th>\n",
       "    <th>0.043917</th>\n",
       "    <th>92.146751</th>\n",
       "    <th>97.616768</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>0.040177</th>\n",
       "    <th>21.467142</th>\n",
       "    <th>0.041073</th>\n",
       "    <th>92.287956</th>\n",
       "    <th>97.663429</th>\n",
       "    <th>02:44</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>0.036485</th>\n",
       "    <th>22.281319</th>\n",
       "    <th>0.037790</th>\n",
       "    <th>92.447952</th>\n",
       "    <th>97.715370</th>\n",
       "    <th>02:46</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>0.035725</th>\n",
       "    <th>22.630697</th>\n",
       "    <th>0.035189</th>\n",
       "    <th>92.582191</th>\n",
       "    <th>97.751213</th>\n",
       "    <th>02:44</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>0.030994</th>\n",
       "    <th>23.295933</th>\n",
       "    <th>0.032528</th>\n",
       "    <th>92.682335</th>\n",
       "    <th>97.790382</th>\n",
       "    <th>02:45</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>0.030128</th>\n",
       "    <th>23.681437</th>\n",
       "    <th>0.030570</th>\n",
       "    <th>92.813004</th>\n",
       "    <th>97.832253</th>\n",
       "    <th>02:44</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>0.026888</th>\n",
       "    <th>23.875751</th>\n",
       "    <th>0.028902</th>\n",
       "    <th>92.918770</th>\n",
       "    <th>97.862045</th>\n",
       "    <th>02:45</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>0.027455</th>\n",
       "    <th>24.270645</th>\n",
       "    <th>0.027668</th>\n",
       "    <th>93.012779</th>\n",
       "    <th>97.888527</th>\n",
       "    <th>02:44</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>0.026799</th>\n",
       "    <th>24.366652</th>\n",
       "    <th>0.026853</th>\n",
       "    <th>93.103302</th>\n",
       "    <th>97.910088</th>\n",
       "    <th>02:45</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>0.026561</th>\n",
       "    <th>24.410763</th>\n",
       "    <th>0.026457</th>\n",
       "    <th>93.184769</th>\n",
       "    <th>97.939629</th>\n",
       "    <th>02:44</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******EPOCH 0 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.14, 'bed': 0.01, 'bench': 0.25, 'bookshelf': 0.02, 'bottle': 0.01, 'chair': 0.01, 'cone': 0.1, 'cup': 0.35, 'curtain': 0.05, 'desk': 0.15, 'door': 0.1, 'dresser': 0.06, 'flower_pot': 0.85, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.19, 'piano': 0.13, 'plant': 0.11, 'radio': 0.4, 'range_hood': 0.09, 'sink': 0.35, 'sofa': 0.02, 'stairs': 0.15, 'stool': 0.3, 'table': 0.14, 'tent': 0.1, 'tv_stand': 0.13, 'vase': 0.17, 'wardrobe': 0.5, 'xbox': 0.4} 33\n",
      "\n",
      "*******EPOCH 1 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.08, 'bed': 0.01, 'bench': 0.2, 'bookshelf': 0.03, 'bottle': 0.01, 'chair': 0.01, 'cone': 0.1, 'cup': 0.3, 'curtain': 0.05, 'desk': 0.19, 'dresser': 0.06, 'flower_pot': 0.85, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.21, 'piano': 0.15, 'plant': 0.11, 'radio': 0.45, 'range_hood': 0.13, 'sink': 0.3, 'sofa': 0.02, 'stairs': 0.15, 'stool': 0.3, 'table': 0.15, 'tent': 0.15, 'tv_stand': 0.12, 'vase': 0.19, 'wardrobe': 0.4, 'xbox': 0.35} 32\n",
      "\n",
      "*******EPOCH 2 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.06, 'bench': 0.2, 'bookshelf': 0.03, 'bottle': 0.01, 'chair': 0.01, 'cone': 0.1, 'cup': 0.3, 'curtain': 0.1, 'desk': 0.09, 'dresser': 0.03, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.3, 'piano': 0.06, 'plant': 0.19, 'radio': 0.4, 'range_hood': 0.1, 'sink': 0.35, 'sofa': 0.01, 'stairs': 0.25, 'stool': 0.25, 'table': 0.18, 'tent': 0.1, 'tv_stand': 0.17, 'vase': 0.16, 'wardrobe': 0.5, 'xbox': 0.45} 31\n",
      "\n",
      "*******EPOCH 3 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.08, 'bed': 0.02, 'bench': 0.2, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.15, 'chair': 0.01, 'cone': 0.1, 'cup': 0.25, 'curtain': 0.05, 'desk': 0.13, 'dresser': 0.03, 'flower_pot': 0.95, 'glass_box': 0.03, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.09, 'monitor': 0.03, 'night_stand': 0.23, 'piano': 0.08, 'plant': 0.07, 'radio': 0.5, 'range_hood': 0.07, 'sink': 0.35, 'sofa': 0.02, 'stairs': 0.1, 'stool': 0.3, 'table': 0.14, 'tv_stand': 0.14, 'vase': 0.16, 'wardrobe': 0.4, 'xbox': 0.35} 32\n",
      "\n",
      "*******EPOCH 4 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.06, 'bench': 0.2, 'bookshelf': 0.02, 'bottle': 0.01, 'bowl': 0.05, 'cone': 0.1, 'cup': 0.35, 'curtain': 0.05, 'desk': 0.07, 'door': 0.05, 'dresser': 0.14, 'flower_pot': 0.85, 'glass_box': 0.03, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.12, 'piano': 0.11, 'plant': 0.06, 'radio': 0.55, 'range_hood': 0.05, 'sink': 0.2, 'sofa': 0.02, 'stairs': 0.2, 'stool': 0.15, 'table': 0.17, 'tent': 0.1, 'tv_stand': 0.06, 'vase': 0.16, 'wardrobe': 0.8, 'xbox': 0.3} 32\n",
      "\n",
      "*******EPOCH 5 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.08, 'bench': 0.15, 'bookshelf': 0.05, 'bottle': 0.01, 'bowl': 0.15, 'cone': 0.05, 'cup': 0.25, 'curtain': 0.05, 'desk': 0.05, 'dresser': 0.03, 'flower_pot': 0.8, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.04, 'monitor': 0.02, 'night_stand': 0.22, 'piano': 0.11, 'plant': 0.09, 'radio': 0.35, 'range_hood': 0.17, 'sink': 0.4, 'sofa': 0.01, 'stairs': 0.2, 'stool': 0.15, 'table': 0.19, 'tent': 0.1, 'tv_stand': 0.06, 'vase': 0.16, 'wardrobe': 0.15, 'xbox': 0.4} 31\n",
      "\n",
      "*******EPOCH 6 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.06, 'bench': 0.15, 'bookshelf': 0.01, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.35, 'curtain': 0.05, 'desk': 0.09, 'door': 0.05, 'dresser': 0.07, 'flower_pot': 0.65, 'glass_box': 0.04, 'guitar': 0.01, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.09, 'monitor': 0.01, 'night_stand': 0.23, 'person': 0.05, 'piano': 0.1, 'plant': 0.15, 'radio': 0.3, 'range_hood': 0.05, 'sink': 0.25, 'sofa': 0.02, 'stairs': 0.05, 'stool': 0.2, 'table': 0.1, 'tent': 0.05, 'tv_stand': 0.13, 'vase': 0.13, 'wardrobe': 0.55, 'xbox': 0.3} 33\n",
      "\n",
      "*******EPOCH 7 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.14, 'bed': 0.01, 'bench': 0.15, 'bottle': 0.01, 'bowl': 0.05, 'chair': 0.01, 'cone': 0.05, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.13, 'door': 0.05, 'dresser': 0.02, 'flower_pot': 0.5, 'glass_box': 0.04, 'guitar': 0.01, 'keyboard': 0.05, 'lamp': 0.2, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.09, 'piano': 0.06, 'plant': 0.14, 'radio': 0.25, 'range_hood': 0.08, 'sink': 0.2, 'sofa': 0.02, 'stairs': 0.1, 'stool': 0.15, 'table': 0.11, 'tent': 0.05, 'tv_stand': 0.15, 'vase': 0.2, 'wardrobe': 0.5, 'xbox': 0.3} 34\n",
      "\n",
      "*******EPOCH 8 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.06, 'bench': 0.15, 'bookshelf': 0.01, 'bottle': 0.02, 'bowl': 0.15, 'cone': 0.05, 'cup': 0.15, 'curtain': 0.05, 'desk': 0.08, 'door': 0.05, 'dresser': 0.02, 'flower_pot': 0.85, 'glass_box': 0.07, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.14, 'piano': 0.05, 'plant': 0.04, 'radio': 0.35, 'range_hood': 0.04, 'sink': 0.35, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.2, 'table': 0.12, 'tent': 0.1, 'tv_stand': 0.14, 'vase': 0.12, 'wardrobe': 0.65, 'xbox': 0.4} 32\n",
      "\n",
      "*******EPOCH 9 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.15, 'curtain': 0.05, 'desk': 0.05, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.12, 'piano': 0.08, 'plant': 0.09, 'radio': 0.5, 'range_hood': 0.06, 'sink': 0.25, 'sofa': 0.02, 'stairs': 0.1, 'stool': 0.2, 'table': 0.16, 'tent': 0.05, 'tv_stand': 0.04, 'vase': 0.14, 'wardrobe': 0.25, 'xbox': 0.35} 29\n",
      "\n",
      "*******EPOCH 10 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.2, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.25, 'desk': 0.05, 'flower_pot': 0.5, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.23, 'person': 0.05, 'piano': 0.11, 'plant': 0.16, 'radio': 0.3, 'range_hood': 0.06, 'sink': 0.2, 'sofa': 0.01, 'stairs': 0.15, 'stool': 0.2, 'table': 0.13, 'tent': 0.05, 'tv_stand': 0.06, 'vase': 0.1, 'wardrobe': 0.4, 'xbox': 0.3} 29\n",
      "\n",
      "*******EPOCH 11 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.08, 'bench': 0.2, 'bookshelf': 0.02, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.25, 'curtain': 0.05, 'desk': 0.09, 'door': 0.05, 'dresser': 0.01, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.14, 'piano': 0.05, 'plant': 0.08, 'radio': 0.3, 'range_hood': 0.05, 'sink': 0.3, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.2, 'table': 0.11, 'tent': 0.05, 'tv_stand': 0.06, 'vase': 0.14, 'wardrobe': 0.25, 'xbox': 0.3} 31\n",
      "\n",
      "*******EPOCH 12 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.01, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.02, 'door': 0.05, 'flower_pot': 0.8, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.09, 'monitor': 0.02, 'night_stand': 0.1, 'piano': 0.05, 'plant': 0.06, 'radio': 0.25, 'range_hood': 0.05, 'sink': 0.15, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.2, 'table': 0.18, 'tent': 0.05, 'tv_stand': 0.07, 'vase': 0.1, 'wardrobe': 0.4, 'xbox': 0.2} 30\n",
      "\n",
      "*******EPOCH 13 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.25, 'curtain': 0.05, 'desk': 0.06, 'door': 0.05, 'flower_pot': 0.4, 'glass_box': 0.04, 'guitar': 0.01, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.08, 'monitor': 0.02, 'night_stand': 0.12, 'person': 0.05, 'piano': 0.06, 'plant': 0.14, 'radio': 0.15, 'range_hood': 0.06, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.2, 'table': 0.11, 'tent': 0.05, 'tv_stand': 0.05, 'vase': 0.15, 'wardrobe': 0.25, 'xbox': 0.25} 32\n",
      "\n",
      "*******EPOCH 14 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.35, 'curtain': 0.05, 'desk': 0.08, 'door': 0.05, 'flower_pot': 0.8, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.07, 'monitor': 0.02, 'night_stand': 0.09, 'piano': 0.05, 'plant': 0.07, 'radio': 0.25, 'range_hood': 0.05, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.2, 'table': 0.11, 'tent': 0.05, 'tv_stand': 0.08, 'vase': 0.12, 'wardrobe': 0.5, 'xbox': 0.3} 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******EPOCH 15 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.05, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.08, 'door': 0.05, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.07, 'monitor': 0.02, 'night_stand': 0.08, 'piano': 0.06, 'plant': 0.07, 'radio': 0.25, 'range_hood': 0.06, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.15, 'table': 0.11, 'tent': 0.05, 'tv_stand': 0.04, 'vase': 0.1, 'wardrobe': 0.25, 'xbox': 0.2} 30\n",
      "\n",
      "*******EPOCH 16 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.07, 'door': 0.05, 'flower_pot': 0.55, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.08, 'monitor': 0.02, 'night_stand': 0.12, 'piano': 0.06, 'plant': 0.12, 'radio': 0.2, 'range_hood': 0.06, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.15, 'table': 0.09, 'tent': 0.05, 'tv_stand': 0.04, 'vase': 0.11, 'wardrobe': 0.25, 'xbox': 0.3} 30\n",
      "\n",
      "*******EPOCH 17 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.07, 'door': 0.05, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.07, 'monitor': 0.02, 'night_stand': 0.12, 'piano': 0.06, 'plant': 0.08, 'radio': 0.25, 'range_hood': 0.06, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.15, 'table': 0.1, 'tent': 0.05, 'tv_stand': 0.04, 'vase': 0.12, 'wardrobe': 0.3, 'xbox': 0.25} 30\n",
      "\n",
      "*******EPOCH 18 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.07, 'door': 0.05, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.08, 'monitor': 0.02, 'night_stand': 0.1, 'piano': 0.06, 'plant': 0.07, 'radio': 0.2, 'range_hood': 0.06, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.15, 'table': 0.1, 'tent': 0.05, 'tv_stand': 0.04, 'vase': 0.11, 'wardrobe': 0.3, 'xbox': 0.25} 30\n",
      "\n",
      "*******EPOCH 19 ********\n",
      "\n",
      "Error rate by class:\n",
      "{'bathtub': 0.04, 'bench': 0.15, 'bookshelf': 0.03, 'bottle': 0.01, 'bowl': 0.1, 'cup': 0.2, 'curtain': 0.05, 'desk': 0.07, 'door': 0.05, 'flower_pot': 0.65, 'glass_box': 0.04, 'keyboard': 0.05, 'lamp': 0.15, 'mantel': 0.07, 'monitor': 0.02, 'night_stand': 0.12, 'piano': 0.06, 'plant': 0.08, 'radio': 0.2, 'range_hood': 0.06, 'sink': 0.1, 'sofa': 0.01, 'stairs': 0.05, 'stool': 0.15, 'table': 0.1, 'tent': 0.05, 'tv_stand': 0.04, 'vase': 0.12, 'wardrobe': 0.2, 'xbox': 0.25} 30\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, slice(1e-6, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save('stage_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage_2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict class and pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([PosixPath('ModelNet40_20/test/vase/vase_0539_001.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_002.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_003.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_004.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_005.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_006.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_007.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_008.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_009.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_010.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_011.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_012.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_013.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_014.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_015.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_016.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_017.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_018.png'),\n",
       "       PosixPath('ModelNet40_20/test/vase/vase_0539_019.png'), PosixPath('ModelNet40_20/test/vase/vase_0539_020.png')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.valid_ds.x.items[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(path/'test/night_stand/night_stand_0201_002.png')\n",
    "batch = learn.data.one_item(img)\n",
    "xb, yb = batch\n",
    "img = open_image(path/'test/night_stand/night_stand_0201_019.png')\n",
    "batch = learn.data.one_item(img)\n",
    "xb1, yb = batch\n",
    "xb = torch.cat((xb, xb1), dim=0)\n",
    "xb = [xb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: night_stand -- pose: 18\n",
      "prediction: night_stand -- pose: 15\n"
     ]
    }
   ],
   "source": [
    "out = learn.model(*xb)\n",
    "batch_size = xb[0].size(0)\n",
    "# print('batch_size: ', batch_size)\n",
    "num_classes = int(out.size( 1 )/ nview) - 1\n",
    "# print('num_classes: ', num_classes)\n",
    "out = out.view( -1, num_classes + 1 )\n",
    "out = torch.nn.functional.log_softmax( out, dim=1 )\n",
    "out = out[ :, :-1 ] - torch.t( out[ :, -1 ].repeat(1, out.size(1)-1).view(out.size(1)-1, -1) )\n",
    "for i in range(batch_size): \n",
    "    maxVals, idxs = torch.max(out[i*nview:i*nview+nview], dim=1)\n",
    "    print('prediction:', classes[idxs[torch.argmax(maxVals)].item()], '-- pose:', torch.argmax(maxVals).item())\n",
    "#     print('pose: ', torch.argmax(maxVals).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7,  2,  6,  1,  5,  0,  4, 13, 15, 12, 14, 18, 19, 16, 17, 11,  9, 10,  8])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show validation views after calculating the \"best view\" for each group of 20 images\n",
    "vimg = data.valid_ds.x.items\n",
    "imgs = vimg[100:120]\n",
    "vcand[viewOrd[5]]\n",
    "\n",
    "fig=plt.figure(figsize=(30, 8))\n",
    "columns = 10\n",
    "rows = 2\n",
    "j = 1\n",
    "for i in vcand[viewOrd[4]]:\n",
    "    img = PIL.Image.open(imgs[i])\n",
    "    fig.add_subplot(rows, columns, j)\n",
    "    j += 1\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3b6ef5ec6f96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, item, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMultiCategoryProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCategoryProcessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "learn.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
